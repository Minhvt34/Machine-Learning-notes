{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyQyoArvCF76DPCB1jVsKU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minhvt34/Machine-Learning-notes/blob/main/DL_Interviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOL-4\n",
        "For a fixed number of observations in a data set, introducing more variables normally generates a model that has a better fit to the data $\\implies$ True, however, when an excessive and unnecessary number of variables is used in a logistic regression model, a phenomena commonly referred to as \"overfitting\". Therefore, it is important that a logistic regression model does not start training with more variables than is justified for the given number of observations."
      ],
      "metadata": {
        "id": "7EVKHGbFvZpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOL-5\n",
        "The odds of sucess are defined as the ratio between the probability of success $p \\in [0, 1]$ and the probability of failure $1 - p$. Formally: \\\\\n",
        "$Odds(p) ≡ \\left(\\frac{p}{1 - p}\\right)$\n",
        "\n",
        "For instace, assuming the probability of success of an event is p = 0.7. Then, in our example, the odds are 7/3, or 2.333 to 1. Natually, in the case of equal probabilities where p = 0.5, the odds of success if 1 to 1."
      ],
      "metadata": {
        "id": "IXBBNPiCwhwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOL-6\n",
        "1. An interaction is the product of two single predictor variables implying a non-additive effect.\n",
        "2. The simplest interaction model includes a predictor variable formed by multiplying two ordinary predictors. Let us assume two variables X and Z. Then, the logistic regression model that employs the simplest form of interaction follows: \\\\\n",
        "$β_0 + β_1X + β_2Z + β_3XZ$\n",
        "where the coefficient for the interaction term XZ is represented by predictor β_3.\n",
        "\n",
        "3. For testing the contribution of an interaction, two principal methods are commonly employed; the Wald chi-squared test or a likelihood ratio test between the model with and without the interaction term.\n",
        "\n",
        "Note: How does interaction related to information theory? What added value does it employ to enhance model performance?"
      ],
      "metadata": {
        "id": "TNycOGmyyXYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOL-8\n",
        "In the case of logistic regression, the response variable is the log of the odds of being classified in a group of binary or multi-class reponses.\n",
        "\n",
        "This definition essentially demonstrates that odds can take the form of a vector."
      ],
      "metadata": {
        "id": "f54knUl88llJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOL-9\n",
        "When a transformation to the response variable is applied, it yields a probability distribution over the output classes, which is bounded between 0 and 1; this transformation can be employed i several ways, e.g., a softmax layer, the signmoid function or classic normalization. This representation facilitates a soft-decision by the logistic regression model, which permits construction of probability-based processes over the predictions of the model \n",
        "\n",
        "Note: what are the pros and cons of each of the three aforementioned transformations."
      ],
      "metadata": {
        "id": "a6DOS5LI-E9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOL-10\n",
        "\n",
        "Minimizing the negative log likelihood also means maximizing the likelihood of selecting the correct class."
      ],
      "metadata": {
        "id": "9SAjkcNTA_3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOL-14\n",
        "A binary logistic regression GLM consists of three components:\n",
        "1. Random component: refers to the probability distribution of the response variable (Y), e.g., binomial distributn for Y in the binary logistic regression, which takes on the values Y = 0 or Y = 1.\n",
        "\n",
        "2. Systematic component: describes the explanatory variables:\n",
        "$(X_1, X_2,...)$ as a combination of linear predictors. The binary case does not constrain these variables to any degree.\n",
        "\n",
        "3. Link function: specifies the link between random and systematic components. It says how the expected value of the response relates to the linear predictor of explanatory variables.\n",
        "Note: Assume that Y denotes whether a human voice activity was detected $(Y = 1)$ or not $(Y = 0)$ in a give time frame. Propose two systematic components and a link function adjusted for this task."
      ],
      "metadata": {
        "id": "frt9n_EwD4_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOL-16\n",
        "\n",
        "The logit function is defined as:\n",
        "$z(p) = logit(p) = log(\\frac{p}{1-p})$ for any $p \\in [0, 1]$. A simple set of algebraic equations yeilds the inverse relation:\n",
        "$p(z) = \\frac{exp z}{1 + exp z}$ which exactly describes the relation between the output and input of the logistic function, aldo known as the signmoid."
      ],
      "metadata": {
        "id": "1lJw5Ym5HAVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOL-19\n",
        "\n",
        "1. Tumour eradication (Y) is the response variable and cancer type (X) is the explanatory variable.\n",
        "\n",
        "2. Relative risk (RR) is the ratio of risk of an event in one group (e.g., exposed group) versus the risk of the event in the other group (e.g., non-exposed group). The odds ratio (OR) is the ratio of odds of an event in one group versus the odds of the event in the other group."
      ],
      "metadata": {
        "id": "V08s8CnxJI0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def Ver003(x): \\\\\n",
        "  return 1 / (1 + np.exp(-(np.clip(x, -709, None))))\n",
        "\n",
        "  this method is more stability because np.clip() function help to limit the boundary. By specifying the minimm and maximum values in the argument, the out-of-range values are replaced with those values."
      ],
      "metadata": {
        "id": "kzAN69rmW8Ns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRB-158\n",
        "\\\\\n",
        "Unlike CNN architectures such as AlexNet or VGG, ResNet does not have any hidden FC layer. True, the ResNet architecture terminates with a global average pooling layer followed by a K-way FC layer with a softmax activation, where K is the number of classes (ImageNet has 1000 classes). Therefore, the ResNet has no hidden FC layers."
      ],
      "metadata": {
        "id": "ExdRL5RmXgZ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xdz7FNjhvYyM"
      },
      "outputs": [],
      "source": []
    }
  ]
}